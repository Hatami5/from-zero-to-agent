complete local setup tutorial that uses both Ollama and **Hugging Face Transformers** side by side.

You’ll end up with a single Python script that can run the **same prompt through both local systems, so you can compare their performance and outputs.

---

 Overview

You will:

1. Install everything
2. Run an Ollama model locally
3. Run a Hugging Face Transformers model locally
4. Compare both outputs
5. (Optionally) integrate with `smolagents` for automated reasoning + code execution

---

Step 1. Install Prerequisites

 Terminal commands

```bash
# Core dependencies
pip install smolagents ollama transformers torch matplotlib
```

If you want optional speedups:

```bash
pip install accelerate bitsandbytes
```

---

## ⚙️ Step 2. Set Up Ollama

### 1. Install Ollama

Go to → [https://ollama.ai/download](https://ollama.ai/download)
Install for your OS (Windows, macOS, or Linux).

### 2. Pull a model

Example:

```bash
ollama pull llama3.1
```

This downloads the model to your local machine.

### 3. Test it

```bash
ollama run llama3.1
```

You should be able to chat directly in your terminal.

---

 Step 3. Hugging Face Transformers Setup

We’ll use **Mistral 7B Instruct** as an example (a good balance between size and quality).

```bash
pip install transformers torch
```

Then, in Python:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",       # Auto-uses GPU if available
    torch_dtype="auto"       # Automatically select best precision
)
```

---

 Step 4. One Script — Compare Ollama vs Transformers

Save this as `compare_local_llms.py`:

```python
import ollama
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

prompt = "Write Python code to plot temperature change over 5 years."

# --- Ollama ---
print("=== Ollama (Llama 3.1) ===")
ollama_response = ollama.chat(model="llama3.1", messages=[
    {"role": "user", "content": prompt}
])
print(ollama_response["message"]["content"])

# --- Hugging Face Transformers ---
print("\n=== Hugging Face (Mistral 7B) ===")
model_id = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(text)
```

 Run it:

```bash
python compare_local_llms.py
```

You’ll get both outputs printed in your terminal.

---

 Step 5. Bonus — Use smolagents with Both

Now, let’s use **smolagents** to let each model **run reasoning + Python code** to *actually generate the plot*.

```python
from smolagents import CodeAgent, LocalModel

prompt = "Plot a graph showing temperature change over 5 years."

# --- Agent with Ollama ---
agent_ollama = CodeAgent(
    model="ollama/llama3.1",
    tools=["python"],
)
print("\n=== Running with Ollama Agent ===")
print(agent_ollama.run(prompt))

# --- Agent with Hugging Face Transformers ---
local_model = LocalModel(model_id="mistralai/Mistral-7B-Instruct-v0.2")

agent_hf = CodeAgent(
    model=local_model,
    tools=["python"],
)
print("\n=== Running with Hugging Face Agent ===")
print(agent_hf.run(prompt))
```

 This will:

* Use **Ollama locally** (fast & easy)
* Use **Transformers model locally** (fully Python-native)
* Execute any Python code the model generates
* Show (or save) the resulting graph

---

 Step 6. Compare the Two

| Aspect            | Ollama                               | Transformers                            |
| ----------------- | ------------------------------------ | --------------------------------------- |
| Setup             | Super simple                         | Requires model + tokenizer setup        |
| Performance       | Optimized binaries                   | Depends on PyTorch & GPU                |
| Control           | Limited tuning                       | Full control (quantization, finetuning) |
| Integration       | Excellent for apps (e.g. smolagents) | Excellent for ML research               |
| Internet Required | ❌ No                                 | ❌ No (after first download)             |
| Output Speed      | ⚡ Very fast                          | ⚡ Depends on GPU                        |

---

 Step 7. Optional Advanced Tips

 Quantization for Low Memory

If your GPU/CPU has limited RAM:

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.2",
    load_in_4bit=True, device_map="auto"
)
```

This reduces VRAM to ~6–8 GB.

---

 Offline Mode

Once models are downloaded, run without internet:

```bash
export TRANSFORMERS_OFFLINE=1
```

---

 List all Ollama models

```bash
ollama list
```

Remove a model:

```bash
ollama rm llama3.1
```

---

 smolagents + Local AI Combo

You can even add custom tools — e.g., `["python", "file", "browser"]` — so your local AI can read/write files or fetch data.

---

 You Now Have:

* Ollama (simple, fast, local inference)
* Transformers (deep control, full offline ML)
* smolagents (autonomous reasoning + code execution)
* A comparison framework to test both

---

Would you like me to show you how to make the **agent automatically save the generated plot as an image file** (instead of just displaying it)?

